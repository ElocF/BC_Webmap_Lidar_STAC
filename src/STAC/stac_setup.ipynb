{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STAC Catalog Setup\n",
    "CFolkers\n",
    "Geospatial Services \n",
    "2024 02 12\n",
    "\n",
    "modified from https://github.com/stac-utils/pystac/blob/8079dd3c0cbe8f6f9e48f499ea90f6a5798eaeab/docs/tutorials/how-to-create-stac-catalogs.ipynb\n",
    "and https://github.com/stac-extensions/pointcloud/blob/main/examples/pdal-to-stac.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading dot env...\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import logging\n",
    "import constants\n",
    "import boto3\n",
    "from botocore.exceptions import ClientError\n",
    "import os\n",
    "from os import path\n",
    "from pathlib import Path\n",
    "import pystac \n",
    "import pdal\n",
    "from osgeo import ogr\n",
    "from osgeo import osr\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<botocore.client.S3 object at 0x7f17da28a310> rczimv\n"
     ]
    }
   ],
   "source": [
    "# use third party object storage to create an S3 Client\n",
    "s3_client = boto3.client(\n",
    "    \"s3\",\n",
    "    endpoint_url=constants.AWS_S3_ENDPOINT,\n",
    "    aws_access_key_id=constants.AWS_ACCESS_KEY_ID,\n",
    "    aws_secret_access_key=constants.AWS_SECRET_ACCESS_KEY,\n",
    ")\n",
    "# for some reason the bucket is adding an extra letter at the end???\n",
    "bucket = constants.AWS_S3_BUCKET\n",
    "\n",
    "print(f\"{s3_client} {bucket}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STAC_LiDAR/PointClouds/bc_092o018_3_2_4_xyes_12_utm10_2018.laz\n",
      "Object Size 23236093\n",
      "STAC_LiDAR/PointClouds/bc_092o018_3_4_2_xyes_12_utm10_2018.laz\n",
      "Object Size 140355729\n",
      "STAC_LiDAR/PointClouds/bc_092o018_3_4_4_xyes_12_utm10_2018.laz\n",
      "Object Size 50122462\n",
      "STAC_LiDAR/PointClouds/bc_092o018_4_1_3_xyes_12_utm10_2018.laz\n",
      "Object Size 95552259\n",
      "STAC_LiDAR/PointClouds/bc_092o018_4_1_4_xyes_12_utm10_2018.laz\n",
      "Object Size 336226672\n",
      "STAC_LiDAR/PointClouds/bc_092o018_4_3_1_xyes_12_utm10_2018.laz\n",
      "Object Size 315611463\n",
      "STAC_LiDAR/PointClouds/bc_092o018_4_3_2_xyes_12_utm10_2018.laz\n",
      "Object Size 354790466\n",
      "STAC_LiDAR/PointClouds/bc_092o018_4_3_3_xyes_12_utm10_2018.laz\n",
      "Object Size 312537985\n",
      "STAC_LiDAR/PointClouds/bc_092o018_4_3_4_xyes_12_utm10_2018.laz\n",
      "Object Size 332078905\n"
     ]
    }
   ],
   "source": [
    "#list .laz objects in bucket\n",
    "object_key=\"STAC_LiDAR/PointClouds/\"\n",
    "laz_objects=[]\n",
    "\n",
    "response = s3_client.list_objects_v2(Bucket=bucket, Prefix=object_key, StartAfter=object_key)\n",
    "\n",
    "if 'Contents' in response:\n",
    "    # Iterate over objects and print their names\n",
    "    for obj in response['Contents']:\n",
    "        laz_objects.append(obj['Key'])\n",
    "        print(obj['Key'])\n",
    "        print(f\"Object Size {obj['Size']}\")\n",
    "else:\n",
    "    print(\"No objects found in the bucket.\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#One way to access the objects is to Create URL to access .laz file \n",
    "url_dict={}\n",
    "for laz in laz_objects:\n",
    "    presigned_url=s3_client.generate_presigned_url('get_object',\n",
    "                                        Params={'Bucket': bucket, 'Key': laz},\n",
    "                                        ExpiresIn=3600)  # Expiration time in seconds (e.g., 1 hour)\n",
    "    # print(presigned_url)\n",
    "    url_dict[laz]=presigned_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "readers.las: Invalid VLR offset - exceeds file size.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 25\u001b[0m\n\u001b[1;32m      6\u001b[0m pipeline \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpipeline\u001b[39m\u001b[38;5;124m\"\u001b[39m: [\n\u001b[1;32m      8\u001b[0m         {\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     22\u001b[0m     ]\n\u001b[1;32m     23\u001b[0m }\n\u001b[1;32m     24\u001b[0m reader \u001b[38;5;241m=\u001b[39m pdal\u001b[38;5;241m.\u001b[39mPipeline(json\u001b[38;5;241m.\u001b[39mdumps(pipeline))\n\u001b[0;32m---> 25\u001b[0m \u001b[43mreader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m boundary \u001b[38;5;241m=\u001b[39m pipeline\u001b[38;5;241m.\u001b[39mmetadata[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfilters.hexbin\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     27\u001b[0m stats \u001b[38;5;241m=\u001b[39m pipeline\u001b[38;5;241m.\u001b[39mmetadata[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfilters.stats\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "\u001b[0;31mRuntimeError\u001b[0m: readers.las: Invalid VLR offset - exceeds file size."
     ]
    }
   ],
   "source": [
    "# not working,\n",
    "# but might be ok once the headers are fixed\n",
    "#attempt to access laz file form s3 via link\n",
    "\n",
    "for key in url_dict:\n",
    "\n",
    "    pipeline = {\n",
    "        \"pipeline\": [\n",
    "            {\n",
    "                \"type\": \"readers.las\",\n",
    "                \"filename\":url_dict[key]\n",
    "            },\n",
    "            {\n",
    "                \"type\": \"filters.hexbin\"\n",
    "            },\n",
    "            {\n",
    "                \"type\": \"filters.stats\"\n",
    "            },\n",
    "            {\n",
    "                \"type\": \"filters.info\"\n",
    "            }\n",
    "            # Add more processing or filters if needed\n",
    "        ]\n",
    "    }\n",
    "    reader = pdal.Pipeline(json.dumps(pipeline))\n",
    "    reader.execute()\n",
    "    boundary = pipeline.metadata['metadata']['filters.hexbin']\n",
    "    stats = pipeline.metadata['metadata']['filters.stats']\n",
    "    info = pipeline.metadata['metadata']['filters.info']\n",
    "    \n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "readers.copc: The first VLR in a COPC file is required to have user_id of 'copc' and this file has 'LASF_Projection'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 12\u001b[0m\n\u001b[1;32m      8\u001b[0m i \u001b[38;5;241m=\u001b[39m pdal\u001b[38;5;241m.\u001b[39mFilter\u001b[38;5;241m.\u001b[39minfo()\n\u001b[1;32m     10\u001b[0m pipeline: pdal\u001b[38;5;241m.\u001b[39mPipeline \u001b[38;5;241m=\u001b[39m r \u001b[38;5;241m|\u001b[39m hb \u001b[38;5;241m|\u001b[39m s \u001b[38;5;241m|\u001b[39m i\n\u001b[0;32m---> 12\u001b[0m count \u001b[38;5;241m=\u001b[39m \u001b[43mpipeline\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m boundary \u001b[38;5;241m=\u001b[39m pipeline\u001b[38;5;241m.\u001b[39mmetadata[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m'\u001b[39m][hb\u001b[38;5;241m.\u001b[39mtype]\n\u001b[1;32m     15\u001b[0m stats \u001b[38;5;241m=\u001b[39m pipeline\u001b[38;5;241m.\u001b[39mmetadata[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m'\u001b[39m][s\u001b[38;5;241m.\u001b[39mtype]\n",
      "\u001b[0;31mRuntimeError\u001b[0m: readers.copc: The first VLR in a COPC file is required to have user_id of 'copc' and this file has 'LASF_Projection'"
     ]
    }
   ],
   "source": [
    "# not working\n",
    "#another attempt to read .laz files from s3 link\n",
    "\n",
    "for key in url_dict.values():\n",
    "    r = pdal.Reader.copc(key)\n",
    "    hb = pdal.Filter.hexbin()\n",
    "    s = pdal.Filter.stats()\n",
    "    i = pdal.Filter.info()\n",
    "\n",
    "    pipeline: pdal.Pipeline = r | hb | s | i\n",
    "\n",
    "    count = pipeline.execute()\n",
    "\n",
    "    boundary = pipeline.metadata['metadata'][hb.type]\n",
    "    stats = pipeline.metadata['metadata'][s.type]\n",
    "    info = pipeline.metadata['metadata'][i.type]\n",
    "    copc = pipeline.metadata['metadata'][r.type]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Object data: STAC_LiDAR/PointClouds/bc_092o018_3_2_4_xyes_12_utm10_2018.laz {'ResponseMetadata': {'RequestId': '8e22ee18:1893b9f8f46:2a87bb:2af5', 'HostId': '63b89ba08864215e81252de20042b4b45c2a70ca0f520af1e730ecefa21e44b3', 'HTTPStatusCode': 200, 'HTTPHeaders': {'date': 'Mon, 26 Feb 2024 19:28:32 GMT', 'server': 'ViPR/1.0', 'x-amz-request-id': '8e22ee18:1893b9f8f46:2a87bb:2af5', 'x-amz-id-2': '63b89ba08864215e81252de20042b4b45c2a70ca0f520af1e730ecefa21e44b3', 'x-amz-version-id': '1708975587179', 'etag': '\"0ac23eae30b35321baa8053e46a54274-3\"', 'last-modified': 'Mon, 26 Feb 2024 19:26:27 GMT', 'x-emc-mtime': '1708975587179', 'content-type': 'application/octet-stream', 'content-length': '23236093'}, 'RetryAttempts': 0}, 'LastModified': datetime.datetime(2024, 2, 26, 19, 26, 27, tzinfo=tzutc()), 'ContentLength': 23236093, 'ETag': '\"0ac23eae30b35321baa8053e46a54274-3\"', 'VersionId': '1708975587179', 'ContentType': 'application/octet-stream', 'Metadata': {}, 'Body': <botocore.response.StreamingBody object at 0x7f176d619150>}\n",
      "Error: Object of type datetime is not JSON serializable\n"
     ]
    }
   ],
   "source": [
    "# not working\n",
    "# another potential way is to use the get_object method \n",
    "for laz in laz_objects:\n",
    "    try:\n",
    "        response = s3_client.get_object(Bucket=bucket, Key=laz)\n",
    "        # Access the object data\n",
    "        object_data = response\n",
    "        print(\"Object data:\",laz,  object_data)\n",
    "        r = pdal.Reader.las(response)\n",
    "        hb = pdal.Filter.hexbin()\n",
    "        s = pdal.Filter.stats()\n",
    "        i = pdal.Filter.info()\n",
    "\n",
    "        pipeline: pdal.Pipeline = r | hb | s | i\n",
    "\n",
    "        count = pipeline.execute()\n",
    "\n",
    "        boundary = pipeline.metadata['metadata'][hb.type]\n",
    "        stats = pipeline.metadata['metadata'][s.type]\n",
    "        info = pipeline.metadata['metadata'][i.type]\n",
    "        copc = pipeline.metadata['metadata'][r.type]\n",
    "    except Exception as e:\n",
    "        print(\"Error:\", e)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def capture_date(pdalinfo):\n",
    "    import datetime\n",
    "    year = pdalinfo['creation_year']\n",
    "    day = pdalinfo['creation_doy']\n",
    "    date = datetime.datetime(int(year), 1, 1) + datetime.timedelta(int(day) - 1 if int(day) > 1 else int(day))\n",
    "    return date.isoformat()+'Z'\n",
    "\n",
    "def convertGeometry(geom, srs):\n",
    "    from osgeo import ogr\n",
    "    from osgeo import osr\n",
    "    in_ref = osr.SpatialReference()\n",
    "    in_ref.SetFromUserInput(srs)\n",
    "    out_ref = osr.SpatialReference()\n",
    "    out_ref.SetFromUserInput('EPSG:4326')\n",
    "\n",
    "    g = ogr.CreateGeometryFromJson(json.dumps(geom))\n",
    "    g.AssignSpatialReference(in_ref)\n",
    "    g.TransformTo(out_ref)\n",
    "    return json.loads(g.ExportToJson())\n",
    "\n",
    "\n",
    "def convertBBox(obj):\n",
    "    output = []\n",
    "    output.append(float(obj['minx']))\n",
    "    output.append(float(obj['miny']))\n",
    "    output.append(float(obj['minz']))\n",
    "    output.append(float(obj['maxx']))\n",
    "    output.append(float(obj['maxy']))\n",
    "    output.append(float(obj['maxz']))\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#download .laz files and read the headers\n",
    "\n",
    "download_loc=r'/home/cfolkers/STAC_LiDAR/PointClouds/'\n",
    "\n",
    "for laz in laz_objects:\n",
    "    output = {}\n",
    "    \n",
    "    laz_donwload=f\"{download_loc}{laz.split('/')[-1]}\"\n",
    "    \n",
    "    if not os.path.exists(laz_donwload):   \n",
    "        s3_client.download_file(bucket, object_key, laz_donwload)\n",
    "    # #fix WKT \n",
    "    # filename = laz_donwload\n",
    "    # with open(filename, \"rb+\") as f:\n",
    "    #     f.seek(6)\n",
    "    #     f.write(bytes([17, 0, 0, 0]))\n",
    "    # print (f\"Fixed:{filename}\")\n",
    "    \n",
    "    # #start pdal pipline\n",
    "    # r = pdal.Reader.las(laz_donwload)\n",
    "    # hb = pdal.Filter.hexbin()\n",
    "    # s = pdal.Filter.stats()\n",
    "    # i = pdal.Filter.info()\n",
    "\n",
    "    # pipeline: pdal.Pipeline = r | hb | s | i\n",
    "\n",
    "    # count = pipeline.execute()\n",
    "\n",
    "    # boundary = pipeline.metadata['metadata'][hb.type]\n",
    "    # stats = pipeline.metadata['metadata'][s.type]\n",
    "    # info = pipeline.metadata['metadata'][i.type]\n",
    "    # copc = pipeline.metadata['metadata'][r.type]\n",
    "    \n",
    "    # try:\n",
    "    #     output['geometry'] = convertGeometry(\n",
    "    #         boundary['boundary_json'],\n",
    "    #         copc['comp_spatialreference']\n",
    "    #     )\n",
    "    # except KeyError:\n",
    "    #     output['geometry'] = stats['bbox']['EPSG:4326']['boundary']\n",
    "\n",
    "    # output['bbox'] = convertBBox(stats['bbox']['EPSG:4326']['bbox'])\n",
    "    # output['id'] = path.basename(filename)\n",
    "    # output['type'] = 'Feature'\n",
    "\n",
    "    # assets = {'data': {'href': filename}}\n",
    "    # properties = {}\n",
    "\n",
    "    # properties['pc:schemas'] = info['schema']['dimensions']\n",
    "    # properties['pc:statistics'] = stats['statistic']\n",
    "    # properties['title'] = \"USGS 3DEP LiDAR\"\n",
    "    # properties['providers'] = [\n",
    "    #     {\n",
    "    #         \"name\": \"USGS\",\n",
    "    #         \"description\": \"United States Geological Survey\",\n",
    "    #         \"roles\": [\n",
    "    #         \"producer\",\n",
    "    #         ],\n",
    "    #         \"url\": \"https://www.usgs.gov\"\n",
    "    #     }\n",
    "    # ]\n",
    "    # properties['pc:type'] = 'lidar' # eopc, lidar, radar, sonar\n",
    "    # try:\n",
    "    #     properties['pc:density'] = boundary['avg_pt_per_sq_unit']\n",
    "    # except KeyError:\n",
    "    #     properties['pc:density'] = 0\n",
    "    # properties['pc:count'] = count\n",
    "\n",
    "    # properties['datetime'] = capture_date(copc)\n",
    "\n",
    "    # output['properties'] = properties\n",
    "    # output['assets'] = assets\n",
    "    # output['stac_extensions'] = ['https://stac-extensions.github.io/pointcloud/v1.0.0/schema.json']\n",
    "    # output['stac_version'] = '1.0.0'\n",
    "\n",
    "    # example_dir = Path(__file__).parent\n",
    "    # out_filename = str(example_dir/'example-autzen.json')\n",
    "\n",
    "    # self_link = {'rel':'self',\"href\":'./example-autzen.json'}\n",
    "    # lic_link = {'rel':'license',\"href\":'https://github.com/PDAL/data/blob/master/LICENSE'}\n",
    "    # output['links'] = [self_link, lic_link]\n",
    "    \n",
    "    # with open(out_filename, 'w') as autzen_out:\n",
    "    #     autzen_out.write(json.dumps(output, sort_keys=True, indent=2,\n",
    "    #                                 separators=(',', ': ')))\n",
    "    \n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "?pystac.Catalog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "catalog = pystac.Catalog(id=\"lidar-test\", description=\"Test catalog for the potential use of STAC to access open LiDAR Data\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stac_tools",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
